# Universidad Peruana de Ciencias Aplicadas  
## 1ASI0404 ‚Äì Inteligencia Artificial  
**Secci√≥n:** 5215  
**Profesor:** Diego Rojas Sihuay  

---

# Trabajo Parcial

### Integrantes

| Apellidos, nombres | C√≥digo |
|--------------------|---------|
| Mendoza Quispe Carlos Fabian | U20231C416 |
| Ibarra Cabrera Camila Adriana | U202317287 | 
| Moncada Olivares El√≠as David | U202315959 | 
| Miranda Cardenas Sofia Gabriel | U20191C439 | 
| Olivera Alvarez Lizbeth Teresita | U201616851 | 
| Huapaya Vargas Daniel Jos√© | U202312230 | 
| Abanto Davila Alvaro Jair Moises | U202313540 | 
| Antonio Moran Jose David | U202223356 | 
| Toledo Mamani Wilber Franz | U202320608 | 
| Rojas S√°nchez Patricia Luc√≠a del Rosario | U202310474 | 

**Facultad de Ingenier√≠a ‚Äì 2025-2**

---

## Roles asignados para el proyecto

| Integrante | Rol (seg√∫n el enunciado) | Tareas |
|-------------|---------------------------|--------|
| Abanto D√°vila, √Ålvaro Jair Mois√©s | Scrum Master | Introducci√≥n y situaci√≥n de contexto real |
| Antonio Mor√°n, Jos√© David | UI/UX Designer | Introducci√≥n y situaci√≥n de contexto real |
| Huapaya Vargas, Daniel Jos√© | Data Engineer | Introducci√≥n y situaci√≥n de contexto real |
| Ibarra Cabrera, Camila Adriana | Data Scientist | Adquisici√≥n de datos |
| Mendoza Quispe, Carlos Fabi√°n | Developer | Experimentos |
| Miranda C√°rdenas, Sof√≠a Gabriel | Data Scientist | Propuesta t√©cnica |
| Moncada Olivares, El√≠as David | Data Engineer | Propuesta t√©cnica |
| Olivera √Ålvarez, Lizbeth Teresita | Developer | Propuesta t√©cnica |
| Rojas S√°nchez, Patricia Luc√≠a del Rosario | Data Engineer | Ingenier√≠a de caracter√≠sticas |
| Toledo Mamani, Wilber Franz | QA Tester | Propuesta t√©cnica |

---

## Introducci√≥n

El **Trastorno del Espectro Autista (TEA)** es una condici√≥n del neurodesarrollo que impacta la comunicaci√≥n, la interacci√≥n social y el comportamiento. Seg√∫n la **Organizaci√≥n Mundial de la Salud (OMS, 2025)**, se estima que **1 de cada 100 ni√±os** en el mundo presenta TEA, lo que plantea un desaf√≠o significativo para la inclusi√≥n educativa y social. Las dificultades en la comunicaci√≥n verbal y la comprensi√≥n de interacciones sociales son barreras importantes que estos ni√±os enfrentan a diario.

Este proyecto se enfoca en el **dise√±o y desarrollo de un chatbot inclusivo**, una herramienta tecnol√≥gica de asistencia dise√±ada espec√≠ficamente para ni√±os con TEA. El objetivo principal es **facilitar la comunicaci√≥n y el aprendizaje** a trav√©s de una interfaz amigable que utiliza pictogramas, opciones visuales y lenguaje sencillo.  Al crear un canal de comunicaci√≥n adaptado a sus necesidades, buscamos **potenciar su autonom√≠a, reducir la ansiedad social y contribuir al desarrollo de sus habilidades comunicativas**, promoviendo as√≠ su inclusi√≥n en diversos contextos.

---

## Descripci√≥n del contexto

El **Trastorno del Espectro Autista (TEA)** representa una realidad tangible en millones de hogares, escuelas y comunidades en todo el mundo. M√°s all√° de la estad√≠stica de la OMS (2025) que indica que 1 de cada 127 ni√±os vive con esta condici√≥n, existe un contexto diario de desaf√≠os comunicativos que impacta profundamente su calidad de vida y su inclusi√≥n social. Un ni√±o con TEA no solo tiene ‚Äúdificultades para comunicarse‚Äù, sino que puede ser incapaz de expresar necesidades b√°sicas como *‚Äútengo hambre‚Äù* o *‚Äúalgo me duele‚Äù*, lo que a menudo deriva en episodios de frustraci√≥n y ansiedad que son mal interpretados como problemas de conducta.

En el entorno educativo, un maestro puede dar una instrucci√≥n simple como *‚Äúsaquen su cuaderno y abran la p√°gina 20‚Äù*, pero para un ni√±o con TEA, esta secuencia de dos pasos puede ser una barrera insuperable sin apoyo visual. Esta desconexi√≥n entre el lenguaje verbal y su procesamiento cognitivo puede llevar a retraso acad√©mico y a un sentimiento de aislamiento respecto a sus compa√±eros. En el √°mbito social, la incapacidad para interpretar el lenguaje no verbal ‚Äîcomo las expresiones faciales o el tono de voz‚Äî dificulta la creaci√≥n de lazos de amistad, dejando a muchos ni√±os al margen de las interacciones grupales.

Los padres y terapeutas emplean herramientas como los **Sistemas Aumentativos y Alternativos de Comunicaci√≥n (SAAC)**, principalmente tarjetas f√≠sicas con pictogramas.  
Si bien son efectivos, estos sistemas son est√°ticos, limitados en vocabulario y poco pr√°cticos para una comunicaci√≥n fluida.  
El contexto real es, por tanto, una brecha persistente entre la necesidad del ni√±o de comunicarse y las herramientas disponibles para hacerlo de manera efectiva, aut√≥noma y adaptada al mundo digital en el que crece. Este proyecto nace de la necesidad de cerrar esa brecha.

---

## Fundamento de la soluci√≥n

La tecnolog√≠a ofrece una oportunidad √∫nica para crear entornos de comunicaci√≥n adaptados a las necesidades de los ni√±os con TEA. A diferencia de las interacciones humanas, que pueden ser impredecibles y abrumadoras, un chatbot bien dise√±ado ofrece un canal de comunicaci√≥n predecible, paciente y libre de juicios. La soluci√≥n propuesta consiste en un asistente conversacional que integra tres pilares fundamentales: lenguaje sencillo, para reducir la carga cognitiva; soporte pictogr√°fico, utilizando el sistema ARASAAC para asociar conceptos con im√°genes claras y consistentes; e interactividad guiada, que ofrece opciones de respuesta para facilitar la construcci√≥n de di√°logos. Este enfoque no busca reemplazar la terapia, sino complementar, proporcionando una herramienta accesible que permite a los ni√±os practicar habilidades comunicativas en un entorno seguro, aumentando su autonom√≠a y confianza.

---

## Objetivos

**Objetivo general:**  
Dise√±ar y desarrollar un prototipo de chatbot que facilite la comunicaci√≥n funcional en ni√±os con TEA mediante una interfaz basada en texto, voz y pictogramas.

**Objetivos espec√≠ficos:**
- Adquirir y preprocesar un conjunto de datos conversacionales relevante y adaptado a las necesidades comunicativas de los ni√±os con TEA.
- Entrenar y evaluar dos modelos de Inteligencia Artificial (uno basado en reglas y otro de aprendizaje profundo) para la gesti√≥n del di√°logo y la generaci√≥n de respuestas.
- Desarrollar una interfaz gr√°fica de usuario (GUI) intuitiva y accesible que integre la entrada y salida de texto, voz y pictogramas.
- Validar la usabilidad y efectividad del prototipo con casos de prueba simulados que reflejan escenarios de la vida real.

---

# Adquisici√≥n de los Datos

## Obtenci√≥n de datos

La base para un modelo de IA conversacional robusto es un conjunto de datos diverso y de alta calidad. El proceso para construir nuestro dataset se centr√≥ en la obtenci√≥n, combinaci√≥n y limpieza de m√∫ltiples fuentes de datos para entrenamiento de modelos de IA en espa√±ol. Para ello, se realiz√≥ una b√∫squeda exhaustiva en plataformas de datasets como Github, Kaggle y Hugging Face. Finalmente, se seleccionaron tres conjuntos de datos de Hugging Face, obteni√©ndose mediante las librer√≠as de pandas y datasets en Google Colab.

Los datasets fuente fueron:

- **Harsit/xnli2.0_train_spanish:** Es parte de la Cross-lingual Natural Language Inference (XNLI) corpus. Se compone de pares de frases (premisa e hip√≥tesis) en espa√±ol. Es √∫til para tareas de comprensi√≥n del lenguaje natural y razonamiento, ya que aporta pares de frases que ayudan al modelo a entender el razonamiento y la inferencia.  Se obtuvo el dataset leyendo la ruta del archivo csv publicado de la fuente original.

  Obtenido de: [https://huggingface.co/datasets/Harsit/xnli2.0_train_spanish](https://huggingface.co/datasets/Harsit/xnli2.0_train_spanish)

    ```python
    # Cargar el dataset "xnli2.0_train_spanish"
    # Fuente: https://huggingface.co/datasets/Harsit/xnli2.0_train_spanish
    
    try:
        df_xnli = pd.read_csv("hf://datasets/Harsit/xnli2.0_train_spanish/spanish_train.csv")
    except Exception as e:
        print(f"Error loading xnli2.0_train_spanish: {e}")
        df_xnli = None
    
    if df_xnli is not None:
        df_xnli = df_xnli[['premise', 'hypothesis']].copy()
        df_xnli = df_xnli.rename(columns={'premise': 'pregunta', 'hypothesis': 'respuesta'})
        df_xnli = df_xnli[['pregunta', 'respuesta']]

- **Benjleite/FairytaleQA-translated-spanish:** Contiene preguntas y respuestas sobre cuentos infantiles en espa√±ol. Es √∫til para tareas de respuesta a preguntas y comprensi√≥n de lectura. Dado que contiene preguntas y respuestas sencillas sobre cuentos infantiles, es un material tem√°tico ideal para nuestro p√∫blico objetivo. El dataset se obtuvo mediante una lectura de json de los datos de entrenamiento de la fuente de origen.

  Obtenido de: [https://huggingface.co/datasets/benjleite/FairytaleQA-translated-spanish](https://huggingface.co/datasets/benjleite/FairytaleQA-translated-spanish)

  ```python
  # Cargar el dataset "FairytaleQA-translated-spanish"
  # Fuente: https://huggingface.co/datasets/benjleite/FairytaleQA-translated-spanish
  
  splits = {'train': 'train.json', 'validation': 'validation.json', 'test': 'test.json'}
  df = pd.read_json("hf://datasets/benjleite/FairytaleQA-translated-spanish/" + splits["train"])
  
  if 'df' in globals() and df is not None:
      df_fairytale = df[['question', 'answer']].copy()
      df_fairytale = df_fairytale.rename(columns={'question': 'pregunta', 'answer': 'respuesta'})
      df_fairytale = df_fairytale[['pregunta', 'respuesta']]
  else:
      df_fairytale = None
      print("FairytaleQA-translated-spanish dataframe not found.")


- **Kukedlc/spanish-train:** Contiene pares de instrucci√≥n y pregunta en espa√±ol (equivalentes a input y output) de conversaciones e interacciones de un chatbot externo. Ofrece un corpus general de instrucciones y respuestas √∫til como conocimiento general del chatbot sobre diversos dominios de temas m√°s avanzados. Asimismo, ofrece una estructura de ejemplo para estructurar las respuestas. Se obtuvieron los datos mediante `load_dataset`, una funci√≥n que permite cargar conjunto de datos de Hugging Face.

  Obtenido de: [https://huggingface.co/datasets/Kukedlc/spanish-train](https://huggingface.co/datasets/Kukedlc/spanish-train)

  ```python
  # Cargar el dataset "spanish-train"
  # Fuente: https://huggingface.co/datasets/Kukedlc/spanish-train
  
  ds = load_dataset("Kukedlc/spanish-train")
  
  if 'ds' in globals() and ds is not None:
      df_spanish_train = ds['train'].to_pandas()
      df_spanish_train = df_spanish_train.rename(columns={'instruction': 'pregunta', 'output': 'respuesta'})
      df_spanish_train = df_spanish_train[['pregunta', 'respuesta']]
  else:
      df_spanish_train = None
      print("spanish-train dataset not found.")

## Descripci√≥n del conjunto de datos

En lugar de partir de un √∫nico archivo, nuestro conjunto de datos final es el resultado de la **combinaci√≥n de tres fuentes distintas** obtenidas desde el **hub de Hugging Face**.  
Este enfoque garantiza una amplia variedad de **estilos de conversaci√≥n, temas y niveles de complejidad**.

Estos datasets se cargaron y sus columnas se **estandarizaron a un formato com√∫n de pregunta y respuesta** antes de ser combinados en un √∫nico archivo.  
De este gran conjunto de datos, se extrajo una **muestra aleatoria** para crear el archivo que se utilizar√° en la fase de desarrollo:  
`combined_dataset_sample_5000.csv`.

## Preprocesamiento de datos

Antes de que los datos pudieran ser utilizados, se aplic√≥ un proceso de **limpieza fundamental** sobre el dataset combinado (de m√°s de **1.3 millones de filas**) para asegurar su calidad e integridad.  
Los pasos fueron los siguientes:

- **Limpieza de Nulos y Espacios en Blanco:** Se eliminaron todas las filas que no conten√≠an valor en la columna *pregunta* o *respuesta*. Adicionalmente, se removieron los espacios en blanco al inicio y al final de cada texto para descartar registros que solo conten√≠an espacios vac√≠os.
  ```python
  # Eliminar filas donde 'pregunta' o 'respuesta' sean null o est√©n vac√≠as
  df_combinado.dropna(subset=['pregunta', 'respuesta'], inplace=True)
  
  # Eliminar espacios iniciales y finales
  df_combinado['pregunta'] = df_combinado['pregunta'].astype(str).str.strip()
  df_combinado['respuesta'] = df_combinado['respuesta'].astype(str).str.strip()
  
  # Hacer una segunda eliminaci√≥n de datos en blanco
  df_combinado = df_combinado[(df_combinado['pregunta'] != '') & (df_combinado['respuesta'] != '')]

- **Eliminaci√≥n de Duplicados:** Se aplic√≥ una funci√≥n para borrar todas las filas que eran copias exactas de otras. Este paso es crucial para evitar que el modelo de IA se sobreajuste a ejemplos repetidos y para garantizar que el conjunto de datos sea lo m√°s variado posible.
  ```python
  # Eliminar duplicados
  df_combinado.drop_duplicates(inplace=True)

- **Limitaci√≥n de extensi√≥n de las cadenas de texto:** Se implement√≥ un filtro para eliminar aquellas cadenas de texto que superaran las **150 palabras**. Dado que el modelo est√° en una etapa inicial, se opt√≥ por trabajar con un dataset limitado y controlado.
  ```python
  # Eliminar filas con m√°s de 150 palabras en 'pregunta' o 'respuesta'
  word_threshold = 150
  df_combinado = df_combinado[
      (df_combinado['pregunta'].apply(lambda x: len(str(x).split())) <= word_threshold) &
      (df_combinado['respuesta'].apply(lambda x: len(str(x).split())) <= word_threshold)
  ]

- **Normalizaci√≥n del texto:** Se estandariz√≥ todo el texto, transformando las cadenas a **min√∫sculas**, eliminando **saltos de l√≠nea (\n)** y **tabulaciones (\t)**, y manejando algunas contracciones (por ejemplo, convertir *‚Äòq‚Äô* en *que*). Este paso es importante para evitar inconsistencias al aplicar los algoritmos y asegurar que los resultados sean coherentes.
  ```python
  def normalize_text(text):
      if isinstance(text, str):
          #Eliminar saltos de l√≠nea y tabulaciones
          text = re.sub(r'[\n\t]', ' ', text)
          #Convertir a min√∫sculas
          text = text.lower()
          #Manejar constracciones m√°s comunes
          text = re.sub(r'\bq\b', ' que', text)
          text = re.sub(r'\btoy\b', ' estoy', text)
          text = re.sub(r'\bd\b', ' de', text)
          text = re.sub(r'\bpa\b', ' para', text)
          text = re.sub(r'\bt\b', ' te', text)
          #Eliminar solo el punto final si existe
          text = re.sub(r'\.$', '', text)
          #Reemplazar comillas dobles por comillas simples
          text = text.replace('"', "'")
          #Eliminar posibles espacios duplicados
          text = re.sub(r'\s+', ' ', text).strip()
          return text
      else:
          return ""

- **Tokenizaci√≥n del texto:** Se emple√≥ un algoritmo de **tokenizaci√≥n** para dividir las cadenas de texto en palabras. Se agregaron nuevas columnas al dataset con los textos tokenizados, lo que proporciona flexibilidad para trabajar tanto con las cadenas completas como con las tokenizadas. Adem√°s, permite aplicar algoritmos de **procesamiento de lenguaje natural (NLP)** con mayor efectividad.
  ```python
  def tokenize_text(text):
      if isinstance(text, str):
          tokens = word_tokenize(text, language='spanish')
          tokens = [t for t in tokens if t not in string.punctuation]
          return tokens
      else:
          return []

Una vez realizados estos procesos, se garantiza que el dataset es de **alta calidad**, sin datos corruptos, redundantes o poco eficientes para la construcci√≥n del modelo. Sin embargo, todav√≠a se tiene un volumen considerable de datos no √≥ptimo para procesar por los algoritmos seleccionados (**828,050 registros**). Por esta raz√≥n, se considerar√° una **muestra final de 6,000 registros aleatorios**, utilizando un **muestreo estratificado** con cada dataset como estrato.

A continuaci√≥n se muestra informaci√≥n sobre el conjunto final de datos obtenido luego de la limpieza realizada.

| **Columna**             | **Tipo de variable** | **Descripci√≥n** |
|--------------------------|----------------------|-----------------|
| **pregunta**             | *string*             | Es la pregunta o enunciado normalizado del dataset original. |
| **respuesta**            | *string*             | Es la respuesta o interpretaci√≥n normalizada correspondiente a la pregunta. |
| **origen**               | *string*             | Dataset de origen del registro, es de tipo categ√≥rico y puede ser ‚Äúxnli‚Äù, ‚Äúfairytale‚Äù o ‚Äúspa_train‚Äù. |
| **pregunta_tokenizada**  | *arreglo*            | Es un vector compuesto de las palabras que componen la pregunta o enunciado tokenizado. |
| **respuesta_tokenizada** | *arreglo*            | Es un vector compuesto de las palabras que componen la respuesta o interpretaci√≥n tokenizada. |

**C√≥digo:** [üîó Parcial - Dataset-Chatbot-IA.ipynb](https://colab.research.google.com/drive/10sadXm8uxeJ6Hcwe1_mPFU0VbCz5NURx?usp=sharing)

